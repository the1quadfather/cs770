The WSU CS 770 (Machine Learning) final group project focused on the recreation of a attention-based neural machine translation model and analysis of its performance.

The group successfully recreated the transformer using open data and compared its performance to mBART on the same dataset (WMT14 German-English).

The primary constraint was: processing time and memory availability across the entire dataset, therefore the entire scales were drastically reduced and this significantly impacted performance across both models.

Included are the attention modules, LSTM testing, and LSTM training files for use on the appropriate datasets, as well as the tokenizers and model weights as pickle and keras files, respectively. 

All datasets, cited works, and other repositories/code are property of their respective owners.
